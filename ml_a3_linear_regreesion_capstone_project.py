# -*- coding: utf-8 -*-
"""ML_A3_Linear_Regreesion_Capstone_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15aniFDoq9Wtz3RIKRsSqZmcGBQRm-WAu

# Linear Regression Implementation and Analysis

**Learning Objectives:**
- Implement production-grade linear regression
- Master data engineering and preprocessing techniques
- Understand model assessment and validation in real-world scenarios
- Learn industry-standard practices for model deployment

**Dataset:** House Sales in King County, USA (21,613 records, 21 features)
- Download from: https://www.kaggle.com/datasets/harlfoxem/housesalesprediction

**Prerequisites:**
- Python fundamentals
- Basic statistics
- NumPy, Pandas, Scikit-learn basics
"""



# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import logging
import sys



from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures


from scipy import stats
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    explained_variance_score
)
from sklearn.linear_model import LinearRegression as SkLearnLR
from sklearn.linear_model import Lasso, Ridge
from typing import Union, Dict, Tuple
import warnings
warnings.filterwarnings('ignore')


# Set random seed for reproducibility
np.random.seed(42)

"""## Section 1: Data Engineering and Preprocessing

### Task 1.1: Data Loading and Initial Exploration
Implement the following functions with proper error handling and logging:
"""

class DataValidator:
    def __init__(self):
        self.validation_results = {}

    def check_missing_values(self, data: pd.DataFrame) -> dict:
        """Check for missing values in the dataset"""
        if not isinstance(data, pd.DataFrame):
            logging.error("Input is not a valid DataFrame.")
            return {}

        missing_values = data.isnull().sum()
        total_missing = missing_values.sum()
        missing_summary = {
            "total_missing": total_missing,
            "missing_per_column": missing_values[missing_values > 0].to_dict()
        }
        logging.info("Missing value check completed.")
        self.validation_results["missing_values"] = missing_summary
        return missing_summary

    def check_data_types(self, data: pd.DataFrame) -> dict:
        """Verify data types of all columns"""
        if not isinstance(data, pd.DataFrame):
            logging.error("Input is not a valid DataFrame.")
            return {}

        data_types = {col: str(dtype) for col, dtype in data.dtypes.items()}
        logging.info("Data type check completed.")
        self.validation_results["data_types"] = data_types
        return data_types

    def check_value_ranges(self, data: pd.DataFrame) -> dict:
        """Check if numerical values are within expected ranges"""
        if not isinstance(data, pd.DataFrame):
            logging.error("Input is not a valid DataFrame.")
            return {}

        numeric_data = data.select_dtypes(include=[np.number])
        range_summary = {
            col: {
                "min": numeric_data[col].min(),
                "max": numeric_data[col].max(),
                "mean": numeric_data[col].mean()
            }
            for col in numeric_data.columns if numeric_data[col].notnull().any()
        }

        logging.info("Value range check completed.")
        self.validation_results["value_ranges"] = range_summary
        return range_summary

"""### Task 1.2: Feature Engineering Pipeline
Create a robust feature engineering pipeline with the following components:
"""

class FeatureEngineer:
    def __init__(self):
        self.feature_stats = {}

    def create_temporal_features(self, data, date_column):
        """
        Extract temporal features from date columns.
        """
        if date_column not in data.columns:
            raise ValueError(f"{date_column} does not exist in the DataFrame.")

        # Ensure the column is in datetime format
        data[date_column] = pd.to_datetime(data[date_column])

        # Extract temporal features
        data["year"] = data[date_column].dt.year
        data["month"] = data[date_column].dt.month
        data["day"] = data[date_column].dt.day
        data["day_of_week"] = data[date_column].dt.dayofweek
        data["is_weekend"] = data[date_column].dt.dayofweek >= 5

        self.feature_stats["temporal_features"] = list(data.columns[-5:])
        return data

    def create_interaction_features(self, data, feature_pairs):
        """
        Create interaction features between specified feature pairs.
        """
        for feature1, feature2 in feature_pairs:
            if feature1 in data.columns and feature2 in data.columns:
                interaction_feature_name = f"{feature1}_x_{feature2}"
                data[interaction_feature_name] = data[feature1] * data[feature2]
                self.feature_stats.setdefault("interaction_features", []).append(interaction_feature_name)
            else:
                raise ValueError(f"One or both features ({feature1}, {feature2}) do not exist in the DataFrame.")
        return data

    def create_polynomial_features(self, data, features, degree=2):
        """
        Create polynomial features for specified columns.
        """
        if not set(features).issubset(data.columns):
            raise ValueError(f"One or more features {features} do not exist in the DataFrame.")

        # Select only the columns specified
        feature_data = data[features]

        # Generate polynomial features
        poly = PolynomialFeatures(degree=degree, include_bias=False)
        poly_features = poly.fit_transform(feature_data)

        # Create column names for the polynomial features
        feature_names = poly.get_feature_names_out(features)

        # Add the new polynomial features to the DataFrame
        poly_df = pd.DataFrame(poly_features, columns=feature_names, index=data.index)
        data = pd.concat([data, poly_df], axis=1)

        self.feature_stats["polynomial_features"] = list(feature_names)
        return data



"""## Section 2: Linear Regression Implementation

### Task 2.1: Implement Linear Regression Class
Implement a production-ready linear regression class with the following components:

"""

class LinearRegression:
    def __init__(self, learning_rate: float = 0.01, n_iterations: int = 1000,
                 regularization: str = None, reg_strength: float = 0.1):
        """
        Initialize Linear Regression model with various regularization options
        """
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.regularization = regularization
        self.reg_strength = reg_strength

        # Initialize appropriate sklearn model based on regularization
        if regularization == 'l1':
            self.model = Lasso(
                alpha=reg_strength,
                max_iter=n_iterations,
                tol=1e-4
            )
        elif regularization == 'l2':
            self.model = Ridge(
                alpha=reg_strength,
                max_iter=n_iterations,
                tol=1e-4
            )
        else:
            self.model = SkLearnLR()

        self.scaler = StandardScaler()
        self.training_history = {'loss': [], 'r2': []}
        self.feature_names = None

    def fit(self, X: np.ndarray, y: np.ndarray, feature_names: list = None) -> 'LinearRegression':
        """
        Train the linear regression model using sklearn
        """
        # Store feature names
        self.feature_names = feature_names if feature_names is not None else [f'X{i}' for i in range(X.shape[1])]

        # Scale features
        X_scaled = self.scaler.fit_transform(X)

        # Fit the model
        self.model.fit(X_scaled, y)

        # Calculate training metrics
        y_pred = self.predict(X)
        self.training_history['loss'].append(mean_squared_error(y, y_pred))
        self.training_history['r2'].append(r2_score(y, y_pred))

        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions using the trained model
        """
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)

    def get_coefficients(self) -> pd.DataFrame:
        """
        Get model coefficients as a DataFrame
        """
        coef_df = pd.DataFrame({
            'Feature': self.feature_names,
            'Coefficient': self.model.coef_
        })
        coef_df = coef_df.sort_values('Coefficient', key=abs, ascending=False)
        return coef_df

    def plot_coefficients(self) -> plt.Figure:
        """
        Plot feature coefficients
        """
        coef_df = self.get_coefficients()

        plt.figure(figsize=(10, 6))
        sns.barplot(data=coef_df, x='Coefficient', y='Feature')
        plt.title('Feature Coefficients')
        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
        plt.tight_layout()

        return plt.gcf()

    def plot_predictions(self, X: np.ndarray, y: np.ndarray) -> plt.Figure:
        """
        Plot actual vs predicted values
        """
        y_pred = self.predict(X)

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        # Actual vs Predicted
        ax1.scatter(y, y_pred, alpha=0.5)
        ax1.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
        ax1.set_xlabel('Actual Values')
        ax1.set_ylabel('Predicted Values')
        ax1.set_title('Actual vs Predicted')

        # Residuals
        residuals = y - y_pred
        ax2.scatter(y_pred, residuals, alpha=0.5)
        ax2.axhline(y=0, color='r', linestyle='--', lw=2)
        ax2.set_xlabel('Predicted Values')
        ax2.set_ylabel('Residuals')
        ax2.set_title('Residual Plot')

        plt.tight_layout()
        return fig

    def get_model_summary(self) -> Dict:
        """
        Get summary of model parameters and performance
        """
        return {
            'coefficients': self.get_coefficients().to_dict('records'),
            'intercept': self.model.intercept_,
            'r2_score': self.training_history['r2'][-1],
            'mse': self.training_history['loss'][-1],
            'regularization': self.regularization,
            'reg_strength': self.reg_strength
        }

"""### Task 2.2: Loss Functions and Regularization

"""

# Ensure LossFunctions class is defined before usage
class LossFunctions:
    @staticmethod
    def mse_loss(y_true: np.ndarray, y_pred: np.ndarray, squared: bool = True) -> float:
        """
        Calculate Mean Squared Error loss using sklearn

        Parameters:
        -----------
        y_true : array-like
            True target values
        y_pred : array-like
            Predicted values
        squared : bool, default=True
            If True returns MSE, if False returns RMSE

        Returns:
        --------
        float
            MSE or RMSE loss value
        """
        return mean_squared_error(y_true, y_pred, squared=squared)

    @staticmethod
    def mae_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """
        Calculate Mean Absolute Error loss using sklearn

        Parameters:
        -----------
        y_true : array-like
            True target values
        y_pred : array-like
            Predicted values

        Returns:
        --------
        float
            MAE loss value
        """
        return mean_absolute_error(y_true, y_pred)

    @staticmethod
    def r2_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """
        Calculate R-squared score using sklearn

        Parameters:
        -----------
        y_true : array-like
            True target values
        y_pred : array-like
            Predicted values

        Returns:
        --------
        float
            R-squared score
        """
        return r2_score(y_true, y_pred)

    @staticmethod
    def explained_variance(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """
        Calculate Explained Variance score using sklearn

        Parameters:
        -----------
        y_true : array-like
            True target values
        y_pred : array-like
            Predicted values

        Returns:
        --------
        float
            Explained variance score
        """
        return explained_variance_score(y_true, y_pred)

    @staticmethod
    def calculate_all_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:
        """
        Calculate multiple evaluation metrics at once

        Parameters:
        -----------
        y_true : array-like
            True target values
        y_pred : array-like
            Predicted values

        Returns:
        --------
        dict
            Dictionary containing all calculated metrics
        """
        metrics = {
            'MSE': LossFunctions.mse_loss(y_true, y_pred),
            'RMSE': LossFunctions.mse_loss(y_true, y_pred, squared=False),
            'MAE': LossFunctions.mae_loss(y_true, y_pred),
            'R2': LossFunctions.r2_score(y_true, y_pred),
            'Explained_Variance': LossFunctions.explained_variance(y_true, y_pred)
        }
        return metrics

"""## Regularisation (Optional)"""

'''class Regularization:
    @staticmethod
    def l1_penalty(weights, alpha):
        """Calculate L1 regularization term

        Parameters:
        -----------
        weights : numpy.ndarray
            Model weights/coefficients
        alpha : float
            Regularization strength parameter

        Returns:
        --------
        float
            L1 regularization term
        """
        pass

    @staticmethod
    def l2_penalty(weights, alpha):
        """Calculate L2 regularization term"""
        # TODO: Implement L2 regularization
        pass

"""## Section 3: Bias-Variance Analysis

### Task 3.1: Implement Learning Curves
"""

class LearningCurveAnalyzer:
    def __init__(self):
        self.train_sizes = None
        self.train_scores = None
        self.validation_scores = None

    def generate_learning_curves(self, model, X, y, train_sizes=np.linspace(0.1, 1.0, 10)):
        """Generate learning curves for model analysis

        Parameters:
        -----------
        model : LinearRegression
            Model instance
        X : numpy.ndarray
            Input features
        y : numpy.ndarray
            Target values
        train_sizes : numpy.ndarray
            Array of training size proportions
        """
        # TODO: Implement learning curve generation
        self.train_sizes, self.train_scores, self.validation_scores = learning_curve(
            model, X, y, train_sizes=train_sizes, cv=5, scoring='neg_mean_squared_error'
        )
        return {
            'train_sizes': self.train_sizes,
            'train_scores': self.train_scores,
            'validation_scores': self.validation_scores
        }


    def plot_learning_curves(self):
        """Plot learning curves with confidence intervals"""
        # TODO: Implement learning curve visualization
        if self.train_scores is None or self.validation_scores is None:
            raise ValueError("Please generate learning curves before plotting.")

        train_mean = -np.mean(self.train_scores, axis=1)
        train_std = np.std(self.train_scores, axis=1)
        validation_mean = -np.mean(self.validation_scores, axis=1)
        validation_std = np.std(self.validation_scores, axis=1)

        plt.figure(figsize=(10, 6))
        plt.plot(self.train_sizes, train_mean, 'o-', color='blue', label='Training Error')
        plt.plot(self.train_sizes, validation_mean, 'o-', color='orange', label='Validation Error')

        # Fill confidence intervals
        plt.fill_between(self.train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')
        plt.fill_between(self.train_sizes, validation_mean - validation_std, validation_mean + validation_std, alpha=0.2, color='orange')

        plt.title('Learning Curves')
        plt.xlabel('Training Size')
        plt.ylabel('Error (MSE)')
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()
        pass

"""# Section 4 ( Putting All together )"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score
import logging

# Load dataset
data = pd.read_csv('//home//kc_house_data.csv')

# Data validation class
class DataValidator:
    def __init__(self, data):
        self.data = data

    def check_missing_values(self):
        missing_values = self.data.isnull().sum()
        return missing_values[missing_values > 0]

    def check_data_types(self):
        return self.data.dtypes

    def check_value_ranges(self):
        return self.data.describe()

# Feature Engineering class
class FeatureEngineer:
    def __init__(self, data):
        self.data = data

    def create_temporal_features(self):
        self.data['date'] = pd.to_datetime(self.data['date'])
        self.data['year'] = self.data['date'].dt.year
        self.data['month'] = self.data['date'].dt.month
        return self.data

    def create_interaction_features(self, feature_pairs):
        for feature1, feature2 in feature_pairs:
            self.data[f'{feature1}_x_{feature2}'] = self.data[feature1] * self.data[feature2]
        return self.data

    def create_polynomial_features(self, features, degree=2):
        poly = PolynomialFeatures(degree=degree, include_bias=False)
        poly_features = poly.fit_transform(self.data[features])
        feature_names = poly.get_feature_names_out(features)
        poly_df = pd.DataFrame(poly_features, columns=feature_names, index=self.data.index)
        self.data = pd.concat([self.data, poly_df], axis=1)
        return self.data

# Data validation
validator = DataValidator(data)
missing_values = validator.check_missing_values()
data_types = validator.check_data_types()
value_ranges = validator.check_value_ranges()

# Feature engineering
engineer = FeatureEngineer(data)
data = engineer.create_temporal_features()
data = engineer.create_interaction_features([('sqft_living', 'sqft_above')])
data = engineer.create_polynomial_features(['sqft_living', 'sqft_above'], degree=2)

# Model Training
features = ['sqft_living', 'sqft_above', 'year', 'month']
X = data[features]
y = data['price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = LinearRegression()
model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)

# Metrics
metrics = {
    'MSE': mean_squared_error(y_test, y_pred),
    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)), # Calculate RMSE using NumPy
    'MAE': mean_absolute_error(y_test, y_pred),
    'R2': r2_score(y_test, y_pred),
    'Explained Variance': explained_variance_score(y_test, y_pred)
}

# Plot predictions
plt.figure(figsize=(10, 5))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Prices')
plt.savefig('//home//predictions.png')

# Save results
pd.DataFrame.from_dict(metrics, orient='index', columns=['Value']).to_csv('//home//metrics.csv')

print("Analysis completed. Metrics and plots saved.")